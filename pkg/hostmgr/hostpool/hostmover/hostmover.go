// Copyright (c) 2019 Uber Technologies, Inc.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//    http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package hostmover

import (
	"context"
	"reflect"
	"strconv"
	"sync"
	"time"

	hpb "github.com/uber/peloton/.gen/peloton/api/v0/host"
	svcpb "github.com/uber/peloton/.gen/peloton/api/v0/host/svc"

	"github.com/uber/peloton/pkg/common/async"
	"github.com/uber/peloton/pkg/common/backoff"
	"github.com/uber/peloton/pkg/common/queue"
	"github.com/uber/peloton/pkg/hostmgr/hostpool/manager"

	"github.com/pborman/uuid"
	"github.com/pkg/errors"
	log "github.com/sirupsen/logrus"
	"github.com/uber-go/tally"
)

// HostMover provides abstraction to move host across pools
type HostMover interface {
	// Async Daemon
	async.Daemon

	// MoveHosts moves hosts from source Pool to destination Pool
	MoveHostsAcrossPools(ctx context.Context,
		numHosts int,
		srcPool,
		destPool string,
	) (RequestId, error)

	// QueryRequestStatus corresponding to requestId returned by
	// MoveHostsAcrossPools
	QueryRequestStatus(ctx context.Context,
		requestId RequestId,
	) MoveStatus
}

// request variables depicts various stages of host-move request
var (
	// Move request has passed the basic validations
	requestAccepted = RequestStatus("ACCEPTED")
	// Move request is currently in the Queue to be processed
	requestInProgress = RequestStatus("IN-PROGRESS")
	// Move request is succeeded
	requestSucceeded = RequestStatus("SUCCEEDED")
	// Move request does not exist
	requestNotFound = RequestStatus("NOT_FOUND")
)

// error variables depicts various errors in host-move request
var (
	errSourcePoolNotExists = errors.New("source pool doesn't exist")
	errDestPoolNotExists   = errors.New("destination pool doesn't exist")
	errEnqueuing           = errors.New("error in enqueuing request")
	errHostNotDown         = errors.New("host is not down")
	errCompleteMaintenance = errors.New("error in completeMaintenance")
)

// Constants
const (
	// Timer for checking reconcile queue
	ReconcileDelay = 5 * time.Second
	// Max parallel workers handling host-move requests
	NumMoveRequestWorkers = 5
	// Sleep duration before checking drain status
	DefaultSleepHostDrainCheck = 2 * time.Second
	// Total time to wait for host to be drained
	TotalWaitForSingleHostDrain = 15 * time.Minute
	// Max size of queue for host-move requests
	MaxRequestQueueSize = 1000 * 100
	// Max size of reconcile queue (failed hosts)
	MaxReconcileQueueSize = 1000
	// Max time to wait for dequeue move request
	MaxRequestDequeueWaitTime = 2 * time.Second
	// Request id key for logging purpose
	RequestIdKey = "requestId"
	// Host name key for logging purpose
	HostNameKey = "hostName"
	// Invalid host value
	InvalidHost = "-1"
	// Maximum time to wait before retry
	RetryableInterval = 15 * time.Second
	// Max retryable attempts for retryable errors
	MaxRetryAttempts = 3
	// Time out for context
	_timeout = 10 * time.Second
)

// TypeDefs for better readability
type HostPool string
type RequestStatus string
type RequestId string

type hostMover struct {
	// daemon object for making host-mover a daemon process
	daemon async.Daemon

	// Dependencies

	// HostSvc Client
	hostClient svcpb.HostServiceYARPCClient

	// HostPool hostPoolManager service
	hostPoolManager manager.HostPoolManager

	// Internal Data structures
	requestsIdToStatusMapping map[RequestId]*MoveStatus

	// Inflight request queue
	queuedRequests queue.Queue

	// Reconcile queue
	reconcileQueue queue.Queue

	// Mutex for locking
	mu sync.RWMutex

	// shutdown channel
	shutDown chan struct{}

	// Metrics
	metrics *Metrics

	// retry policy
	retryPolicy backoff.RetryPolicy

	// num Workers
	numWorkers int

	// total Wait time for single Host
	totalWaitForSingleHostDrain time.Duration
}

// Inflight host-move request.
type InFlightRequest struct {
	// request id generated by our handler
	requestId RequestId
	// List of hosts obtained from scorer
	hosts []string
	// Source pool (from Pool)
	sourcePool HostPool
	// Destination pool (to Pool)
	destinationPool HostPool
}

// Request Status blob.
type MoveStatus struct {
	// Number of hosts moved
	numHostsMoved int32
	// Status of the request
	requestStatus RequestStatus
}

// Constructor
func NewHostMover(
	hostClient svcpb.HostServiceYARPCClient,
	manager manager.HostPoolManager,
	scope tally.Scope,
) HostMover {

	result := &hostMover{
		requestsIdToStatusMapping: make(map[RequestId]*MoveStatus),
		queuedRequests: queue.NewQueue("move-request-queue",
			reflect.TypeOf(InFlightRequest{}), MaxRequestQueueSize),
		reconcileQueue: queue.NewQueue("reconcile-request-queue",
			reflect.TypeOf(""), MaxReconcileQueueSize),
		hostPoolManager:             manager,
		hostClient:                  hostClient,
		numWorkers:                  NumMoveRequestWorkers,
		totalWaitForSingleHostDrain: TotalWaitForSingleHostDrain,
		retryPolicy:                 backoff.NewRetryPolicy(MaxRetryAttempts, RetryableInterval),
		metrics:                     NewMetrics(scope),
		shutDown:                    make(chan struct{}),
	}

	result.daemon = async.NewDaemon("HostMover", result)
	return result
}

// API to move desired number of hosts from sourcePool to destPool
// This will be called by HostPoolManager when if finds that the partition
// corresponding to destination pool is "hot"
// For e.g. if stateless partition is hot; it will ask to move hosts
// from batch partition and vice-versa.
func (hm *hostMover) MoveHostsAcrossPools(
	ctx context.Context,
	numHosts int,
	sourcePool,
	destPool string,
) (RequestId, error) {
	hosts := make([]string, numHosts)

	// Generate random requestId
	requestId := RequestId("move-request:" + uuid.New())

	// ToDo - Integrate with scorer module
	// hosts = resmgrclient.get_hosts_from_scorer(numHosts)

	// Temporary to make sure unit test run.
	// Once scorer is integrated, it could be mocked
	for i := 0; i < numHosts; i++ {
		hosts[i] = "hostName" + strconv.Itoa(i)
	}

	// validate source Pool
	_, err := hm.hostPoolManager.GetPool(sourcePool)
	if err != nil {
		log.WithField("pool", sourcePool).
			WithError(err).
			Error("Source Pool doesn't exist")
		return "", errSourcePoolNotExists
	}

	// validate destination Pool
	_, err = hm.hostPoolManager.GetPool(destPool)
	if err != nil {
		log.WithField("pool", destPool).
			WithError(err).
			Error("Destination Pool doesn't exist")
		return "", errDestPoolNotExists
	}

	if len(hosts) != numHosts {
		log.WithField("pool", sourcePool).
			Info("Scorer not able to find desired number of hosts in the" +
				" sourcePool")
	}

	// Create an inflight request object.
	inFlightRequest := &InFlightRequest{sourcePool: HostPool(sourcePool),
		destinationPool: HostPool(destPool),
		requestId:       requestId,
		hosts:           hosts,
	}

	// Create in-memory cache for the requestId.
	hm.mu.Lock()
	hm.requestsIdToStatusMapping[requestId] = &MoveStatus{
		requestStatus: requestAccepted,
		numHostsMoved: 0,
	}
	hm.mu.Unlock()

	// Put the request into processing queue in retry mode.
	err = backoff.Retry(
		func() error {
			return hm.queuedRequests.Enqueue(inFlightRequest)
		},
		hm.retryPolicy,
		nil,
	)

	// Error. Log and increment the error metrics.
	if err != nil {
		log.WithField(RequestIdKey, requestId).
			WithError(err).
			Error("Error in enqueue the move-request")
		hm.metrics.enqueueErrors.Inc(1)
		return "", errEnqueuing
	}

	// Update metrics for pendingRequest queue
	updateQueueLengthMetrics(&hm.queuedRequests,
		&hm.mu,
		&hm.metrics.pendingMoveRequests)

	return requestId, nil
}

// API to query status of move requests
// This API will be called by HostPoolManager to know about the
// status of the move-request. Ideally the host-mover is responsible for
// triggering ChangePool API, but exposing this API as well so that clients
// can get the status.
func (hm *hostMover) QueryRequestStatus(
	ctx context.Context,
	requestId RequestId,
) MoveStatus {
	hm.mu.Lock()
	defer hm.mu.Unlock()

	moveStatus, exists := hm.requestsIdToStatusMapping[requestId]

	if !exists {
		// If requestId doesn't exist
		moveStatus = &MoveStatus{requestStatus: requestNotFound,
			numHostsMoved: 0}
	}

	return *moveStatus
}

// Internal use.
// Worker threads to dequeue requests and orchestrate draining for
// the selected hosts.
func (hm *hostMover) processMoveRequest() {
	for {
		select {
		case <-hm.shutDown:
			return
		default:
			// Dequeue hostMove request
			item, err := hm.queuedRequests.Dequeue(MaxRequestDequeueWaitTime)
			if err != nil {
				// Error is not because of time-out, log the error
				// Update metrics
				if _, isTimeout := err.(queue.DequeueTimeOutError); !isTimeout {
					log.WithError(err).
						Info("Error in dequeue move-request")
					hm.metrics.dequeueErrors.Inc(1)
				}
				continue
			}

			// Update metrics for pendingRequest queue
			updateQueueLengthMetrics(&hm.queuedRequests,
				&hm.mu,
				&hm.metrics.pendingMoveRequests)

			// Get the request
			moveRequest := item.(*InFlightRequest)

			// Update the request status to be in-progress
			hm.requestStatusUpdate(moveRequest.requestId, requestInProgress)

			wg := sync.WaitGroup{}

			hosts := moveRequest.hosts
			wg.Add(len(hosts))

			// Orchestrate draining of a single host
			for _, host := range hosts {
				go hm.orchestrateSingleHostDrain(moveRequest, host, &wg)
			}

			wg.Wait()

			// Update the request to be succeeded
			hm.requestStatusUpdate(moveRequest.requestId, requestSucceeded)
		}
	}
}

// Internal use. Orchestrate draining of the single host.
func (hm *hostMover) orchestrateSingleHostDrain(
	moveRequest *InFlightRequest,
	hostName string,
	wg *sync.WaitGroup,
) {
	defer wg.Done()

	// ToDo - Take a lock once hostPoolManager provides the semantics
	// lock = hostPoolManager.AcquireHostLock(hostName)
	// lock.Lock
	// defer lock.unLock

	currentHostPool, err := hm.hostPoolManager.GetPoolByHostname(hostName)

	if err != nil {
		log.WithFields(log.Fields{
			HostNameKey: hostName,
		}).Error("Error retrieving the host pool of the host.")
		return
	}

	// Validate if the host's actualPool state == sourcePool
	// This host is returned from scorer, which is having
	// different view of the host
	if HostPool(currentHostPool.ID()) != moveRequest.sourcePool {
		log.WithFields(log.Fields{
			HostNameKey:  hostName,
			"sourcePool": moveRequest.sourcePool,
			"actualPool": currentHostPool.ID(),
		}).Error("Host pool information has been changed." +
			" Actual hostPool is different than the sourcePool")
		return
	}

	req := svcpb.StartMaintenanceRequest{Hostname: hostName}

	// StartMaintenance in retry mode
	err = backoff.Retry(
		func() error {
			ctx, cancelFunc := context.WithTimeout(context.Background(), _timeout)
			defer cancelFunc()

			_, err = hm.hostClient.StartMaintenance(ctx, &req)
			return err
		},
		hm.retryPolicy,
		nil,
	)

	if err != nil {
		// Error in start maintenance. Request failed
		log.WithFields(log.Fields{
			RequestIdKey: moveRequest.requestId,
			HostNameKey:  hostName,
		}).
			WithError(err).
			Error("Error in startMaintenance request")
		hm.metrics.startMaintenanceFail.Inc(1)
		return
	}

	hm.metrics.startMaintenanceSuccess.Inc(1)
	startTime := time.Now()

	// Start Maintenance succeeded. Query Host state till it becomes DOWN
	// before invoking complete maintenance.
	for {
		err = hm.tryCompleteMaintenance(hostName, moveRequest)
		switch err {
		case nil:
			// CompleteMaintenance is successful - update the count of hostsMoved
			hm.incrementMovedHosts(moveRequest.requestId)
			return
		case errCompleteMaintenance:
			hm.metrics.completeMaintenanceFail.Inc(1)
		case errHostNotDown:
		default:
		}

		time.Sleep(DefaultSleepHostDrainCheck)
		nowTime := time.Now()

		// Exit the loop if the host is not DOWN even after
		// waiting for TotalWaitForSingleHostDrain
		if nowTime.After(startTime.Add(hm.totalWaitForSingleHostDrain)) {
			break
		}
	}

	// ToDo
	//  (a) Recovery. What if host becomes DOWN and host-mover crashes
	//  (b) Reconcile DOWN state and UP those hosts. Also clean-up in-memory status
	log.WithFields(log.Fields{
		RequestIdKey: moveRequest.requestId,
		HostNameKey:  hostName,
	}).Info("Host draining not completed. Enqueue to the reconcile queue")

	// Try enqueue in retry mode
	// Critical error.
	err = hm.tryEnqueueForReconcile(hostName)
	if err != nil {
		hm.metrics.reconcileHostFailed.Inc(1)
	}

	return
}

// Internal use. Check if host is DOWN after maintenance.
func (hm *hostMover) isHostDown(
	hostName string,
) bool {
	// Host State filter
	hostStatesFilter := make([]hpb.HostState, 1)
	hostStatesFilter[0] = hpb.HostState_HOST_STATE_DOWN

	queryHostRequest := svcpb.QueryHostsRequest{HostStates: hostStatesFilter}

	// ToDo  - QueryHosts should expose API to get a hostName and return
	//  the host's state
	ctx, cancelFunc := context.WithTimeout(context.Background(), _timeout)
	defer cancelFunc()

	queryHostResponse, err := hm.hostClient.
		QueryHosts(ctx, &queryHostRequest)

	if err != nil {
		log.WithField(HostNameKey, hostName).
			WithError(err).
			Error("Error in querying host status")
		return false
	}

	hostInfos := queryHostResponse.HostInfos
	for _, hostInfo := range hostInfos {
		if hostInfo.Hostname == hostName {
			return true
		}
	}

	return false
}

// Try completing maintenance
func (hm *hostMover) tryCompleteMaintenance(
	hostName string,
	moveRequest *InFlightRequest,
) error {
	if hm.isHostDown(hostName) {
		// ToDo - invoke hostPoolManager.ChangeHostPool once
		// hostPoolManager provides the semantics
		// hostPoolManager.ChangeHostPool(hostName, srcPool, destPool)

		// Host is down - trigger complete maintenance
		completeMaintenanceRequest :=
			svcpb.CompleteMaintenanceRequest{Hostname: hostName}

		// Complete maintenance in retry mode.
		err := backoff.Retry(
			func() error {
				ctx, cancelFunc := context.WithTimeout(context.Background(), _timeout)
				defer cancelFunc()

				_, err := hm.hostClient.CompleteMaintenance(ctx,
					&completeMaintenanceRequest)
				return err
			},
			hm.retryPolicy,
			nil,
		)

		// Error in complete maintenance.
		// Try completing maintenance by putting in reconcile queue.
		if err != nil {
			log.WithFields(log.Fields{
				RequestIdKey: moveRequest.requestId,
				HostNameKey:  hostName,
			}).
				WithError(err).
				Info("Error in completeMaintenance request")
			return errCompleteMaintenance
		}

		// Successfully complete maintenance.
		return nil
	}

	return errHostNotDown
}

// Reconcile thread which will work on the DOWN hosts and trigger UP on them
// via completeMaintenance
// This won't change any pool specific information as the *only*
// responsibility of this is to make sure that host is UP.

// Why is this required?
// During host-move orchestration, host maintenance could not get completed due to
// 1. Error in complete maintenance
// 2. Host not becoming DOWN even after waiting for TotalWaitForSingleHostDrain
// In such cases, host could remain in error condition.
// This module is responsible to prevent such cases.
func (hm *hostMover) reconcileInconsistentHosts() {
	timer := time.NewTimer(time.Duration(0))
	hostName := InvalidHost

	for {
		select {
		case <-hm.shutDown:
			if !timer.Stop() {
				<-timer.C
			}
			return
		case <-timer.C:
			if hostName == InvalidHost {
				// Try dequeue the host from the reconcile queue
				item, err := hm.reconcileQueue.Dequeue(MaxRequestDequeueWaitTime)
				if err != nil {
					// Error is not because of time-out, log the error
					if _, isTimeout := err.(queue.DequeueTimeOutError); !isTimeout {
						log.WithError(err).
							Error("Error in dequeue host from the reconcile queue")
					}
					// do nothing
				} else {
					// Update metrics for reconcile queue
					updateQueueLengthMetrics(&hm.reconcileQueue,
						&hm.mu,
						&hm.metrics.reconcileHostRequests)
					hostName = item.(string)
				}
			}

			if hostName != InvalidHost && hm.isHostDown(hostName) {
				// Host is down - trigger complete maintenance
				completeMaintenanceRequest := svcpb.
					CompleteMaintenanceRequest{Hostname: hostName}

				err := backoff.Retry(
					func() error {
						ctx, cancelFunc := context.WithTimeout(context.Background(), _timeout)
						defer cancelFunc()

						_, err := hm.hostClient.CompleteMaintenance(ctx,
							&completeMaintenanceRequest)
						return err
					}, hm.retryPolicy, nil)

				// Error in complete maintenance.
				if err != nil {
					log.WithFields(log.Fields{HostNameKey: hostName}).
						WithError(err).
						Info("Error triggering complete maintenance during " +
							"reconciliation; enqueue again")
					hm.metrics.completeMaintenanceFail.Inc(1)

					// Try enqueue in retry mode
					err = hm.tryEnqueueForReconcile(hostName)
					if err != nil {
						// Critical error
						hm.metrics.reconcileHostFailed.Inc(1)
					}
				} else {
					// Complete Maintenance succeeded
					log.WithFields(log.Fields{HostNameKey: hostName}).
						Info("Reconciliation successful post maintenance")
					hm.metrics.completeMaintenanceSuccess.Inc(1)
					hm.metrics.reconcileHostSuccess.Inc(1)
					// Try dequeue another host which needs to be changed to UP
					hostName = InvalidHost
				}
			}
		}

		timer.Reset(ReconcileDelay)
	}
}

// Add hosts to reconcile queue which got error in either of
// (a) Complete maintenance
// (b) Draining
func (hm *hostMover) tryEnqueueForReconcile(hostName string) error {
	// Try enqueueing in retry mode
	err := backoff.Retry(
		func() error {
			return hm.reconcileQueue.Enqueue(hostName)
		},
		hm.retryPolicy,
		nil,
	)

	if err != nil {
		log.WithFields(log.Fields{HostNameKey: hostName}).
			WithError(err).
			Error("Critical error in enqueue for reconcile")
		return err
	}

	// Update metrics for pendingRequest queue
	updateQueueLengthMetrics(&hm.reconcileQueue,
		&hm.mu,
		&hm.metrics.reconcileHostRequests)

	return nil
}

// Update metrics for pending queue length.
func updateQueueLengthMetrics(sharedQueue *queue.Queue,
	mu *sync.RWMutex,
	metrics *tally.Gauge,
) {
	mu.Lock()
	defer mu.Unlock()

	pendingQueueLen := (*sharedQueue).Length()

	(*metrics).Update(float64(pendingQueueLen))
}

// Thread safe - Wrapper over request status updates.
func (hm *hostMover) requestStatusUpdate(requestId RequestId,
	status RequestStatus,
) {
	hm.mu.Lock()
	defer hm.mu.Unlock()

	if val, ok := hm.requestsIdToStatusMapping[requestId]; ok {
		val.requestStatus = status
	}
}

// Thread safe - Wrapper over request status host count.
func (hm *hostMover) incrementMovedHosts(requestId RequestId) {
	hm.mu.Lock()
	defer hm.mu.Unlock()

	hm.metrics.completeMaintenanceSuccess.Inc(1)

	if val, ok := hm.requestsIdToStatusMapping[requestId]; ok {
		val.numHostsMoved += 1
	}
}

// Start starts the host mover go routines that starts processing incoming
// requests.
func (hm *hostMover) Start() {
	hm.daemon.Start()
}

// Run method implements runnable from daemon.
func (hm *hostMover) Run(ctx context.Context) error {
	// Spawn worker routines which will process incoming requests
	for worker := 1; worker <= hm.numWorkers; worker++ {
		go hm.processMoveRequest()
	}

	// Spawn reconcile routine
	go hm.reconcileInconsistentHosts()

	return nil
}

// Stop stops the host mover go routine.
func (hm *hostMover) Stop() {
	hm.mu.Lock()
	defer hm.mu.Unlock()

	close(hm.shutDown)

	// Clean up host mover in-memory cache
	hm.requestsIdToStatusMapping = map[RequestId]*MoveStatus{}

	hm.daemon.Stop()
}
